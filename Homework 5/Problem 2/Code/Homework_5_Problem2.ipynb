{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "srMiyDLEEYVR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the text file (upload input.txt first)\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "encoded_text = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
        "\n",
        "# Dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, seq_length):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.data[idx:idx + self.seq_length],\n",
        "            self.data[idx + self.seq_length]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Models (Transformer, LSTM)s\n",
        "class TransformerCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size=64, num_heads=2, num_layers=2, ff_dim=128, dropout=0.1, max_seq_len=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_seq_len, emb_size))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(emb_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = self.embedding(x) + self.pos_embedding[:, :seq_len, :]\n",
        "        x = self.transformer(x)\n",
        "        return self.fc(x[:, -1, :])  # Predict next char\n",
        "\n",
        "class LSTMCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size=64, hidden_size=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :])\n"
      ],
      "metadata": {
        "id": "eULfgIWZEq-k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Train & Evaluate Function\n",
        "def train_model(model, train_loader, val_loader, num_epochs=5, lr=0.003):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = correct / total\n",
        "    return total_loss / len(train_loader), acc, elapsed, sum(p.numel() for p in model.parameters())\n"
      ],
      "metadata": {
        "id": "a7Y5x_ZzEuY3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Basic Transformer + LSTM\n",
        "seq_lengths = [20, 30, 50]\n",
        "\n",
        "for model_type in [\"Transformer\", \"LSTM\"]:\n",
        "    print(f\"\\n======= {model_type} RESULTS =======\")\n",
        "    for seq_len in seq_lengths:\n",
        "        print(f\"\\nTraining {model_type} with seq_length = {seq_len}\")\n",
        "        dataset = CharDataset(encoded_text, seq_len)\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "        train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_set, batch_size=64)\n",
        "\n",
        "        if model_type == \"Transformer\":\n",
        "            model = TransformerCharModel(vocab_size, num_heads=2, num_layers=2)\n",
        "        else:\n",
        "            model = LSTMCharModel(vocab_size)\n",
        "\n",
        "        loss, acc, time_sec, params = train_model(model, train_loader, val_loader)\n",
        "        print(f\"Loss: {loss:.4f}, Acc: {acc:.4f}, Time: {time_sec:.2f}s, Params: {params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O49atgZ5EwaA",
        "outputId": "e16e994b-2c42-40fe-8e9d-9a625875fbb5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======= Transformer RESULTS =======\n",
            "\n",
            "Training Transformer with seq_length = 20\n",
            "Epoch 1, Loss: 2.0966\n",
            "Epoch 2, Loss: 1.9779\n",
            "Epoch 3, Loss: 1.9519\n",
            "Epoch 4, Loss: 1.9439\n",
            "Epoch 5, Loss: 1.9410\n",
            "Loss: 1.9410, Acc: 0.4527, Time: 485.90s, Params: 108097\n",
            "\n",
            "Training Transformer with seq_length = 30\n",
            "Epoch 1, Loss: 2.0928\n",
            "Epoch 2, Loss: 1.9822\n",
            "Epoch 3, Loss: 1.9607\n",
            "Epoch 4, Loss: 1.9427\n",
            "Epoch 5, Loss: 1.9294\n",
            "Loss: 1.9294, Acc: 0.4607, Time: 498.09s, Params: 108097\n",
            "\n",
            "Training Transformer with seq_length = 50\n",
            "Epoch 1, Loss: 2.0933\n",
            "Epoch 2, Loss: 1.9720\n",
            "Epoch 3, Loss: 1.9473\n",
            "Epoch 4, Loss: 1.9395\n",
            "Epoch 5, Loss: 1.9357\n",
            "Loss: 1.9357, Acc: 0.4467, Time: 522.73s, Params: 108097\n",
            "\n",
            "======= LSTM RESULTS =======\n",
            "\n",
            "Training LSTM with seq_length = 20\n",
            "Epoch 1, Loss: 1.6854\n",
            "Epoch 2, Loss: 1.5021\n",
            "Epoch 3, Loss: 1.4703\n",
            "Epoch 4, Loss: 1.4570\n",
            "Epoch 5, Loss: 1.4506\n",
            "Loss: 1.4506, Acc: 0.5494, Time: 237.73s, Params: 243969\n",
            "\n",
            "Training LSTM with seq_length = 30\n",
            "Epoch 1, Loss: 1.6735\n",
            "Epoch 2, Loss: 1.4975\n",
            "Epoch 3, Loss: 1.4669\n",
            "Epoch 4, Loss: 1.4542\n",
            "Epoch 5, Loss: 1.4482\n",
            "Loss: 1.4482, Acc: 0.5537, Time: 237.68s, Params: 243969\n",
            "\n",
            "Training LSTM with seq_length = 50\n",
            "Epoch 1, Loss: 1.6599\n",
            "Epoch 2, Loss: 1.4902\n",
            "Epoch 3, Loss: 1.4619\n",
            "Epoch 4, Loss: 1.4519\n",
            "Epoch 5, Loss: 1.4457\n",
            "Loss: 1.4457, Acc: 0.5543, Time: 249.97s, Params: 243969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Transformer Hyperparameter Grid Search\n",
        "layers_list = [1, 2, 4]\n",
        "heads_list = [2, 4]\n",
        "seq_len = 30\n",
        "\n",
        "print(\"\\n=== Transformer Hyperparameter Comparison ===\")\n",
        "for layers in layers_list:\n",
        "    for heads in heads_list:\n",
        "        print(f\"\\nLayers: {layers}, Heads: {heads}\")\n",
        "        dataset = CharDataset(encoded_text, seq_len)\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "        train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_set, batch_size=64)\n",
        "\n",
        "        model = TransformerCharModel(vocab_size, num_heads=heads, num_layers=layers)\n",
        "        loss, acc, time_sec, params = train_model(model, train_loader, val_loader)\n",
        "        print(f\"Loss: {loss:.4f}, Acc: {acc:.4f}, Time: {time_sec:.2f}s, Params: {params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks10_pNEEueH",
        "outputId": "b0cb78e2-5573-4573-c73f-a84fe67fde50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Transformer Hyperparameter Comparison ===\n",
            "\n",
            "Layers: 1, Heads: 2\n",
            "Epoch 1, Loss: 2.1199\n",
            "Epoch 2, Loss: 2.0297\n",
            "Epoch 3, Loss: 2.0194\n",
            "Epoch 4, Loss: 2.0167\n",
            "Epoch 5, Loss: 2.0177\n",
            "Loss: 2.0177, Acc: 0.4297, Time: 326.03s, Params: 74625\n",
            "\n",
            "Layers: 1, Heads: 4\n",
            "Epoch 1, Loss: 2.0844\n",
            "Epoch 2, Loss: 1.9955\n",
            "Epoch 3, Loss: 1.9963\n",
            "Epoch 4, Loss: 1.9838\n",
            "Epoch 5, Loss: 1.9815\n",
            "Loss: 1.9815, Acc: 0.4333, Time: 330.74s, Params: 74625\n",
            "\n",
            "Layers: 2, Heads: 2\n",
            "Epoch 1, Loss: 2.0929\n",
            "Epoch 2, Loss: 1.9767\n",
            "Epoch 3, Loss: 1.9575\n",
            "Epoch 4, Loss: 1.9470\n",
            "Epoch 5, Loss: 1.9471\n",
            "Loss: 1.9471, Acc: 0.4488, Time: 496.34s, Params: 108097\n",
            "\n",
            "Layers: 2, Heads: 4\n",
            "Epoch 1, Loss: 2.0563\n",
            "Epoch 2, Loss: 1.9500\n",
            "Epoch 3, Loss: 1.9291\n",
            "Epoch 4, Loss: 1.9223\n",
            "Epoch 5, Loss: 1.9219\n",
            "Loss: 1.9219, Acc: 0.4639, Time: 497.07s, Params: 108097\n",
            "\n",
            "Layers: 4, Heads: 2\n",
            "Epoch 1, Loss: 2.1045\n",
            "Epoch 2, Loss: 1.9969\n",
            "Epoch 3, Loss: 2.0015\n",
            "Epoch 4, Loss: 2.0463\n",
            "Epoch 5, Loss: 2.5756\n",
            "Loss: 2.5756, Acc: 0.2929, Time: 835.07s, Params: 175041\n",
            "\n",
            "Layers: 4, Heads: 4\n",
            "Epoch 1, Loss: 2.0826\n",
            "Epoch 2, Loss: 1.9664\n",
            "Epoch 3, Loss: 1.9623\n",
            "Epoch 4, Loss: 2.0584\n",
            "Epoch 5, Loss: 2.3191\n",
            "Loss: 2.3191, Acc: 0.3691, Time: 841.02s, Params: 175041\n"
          ]
        }
      ]
    }
  ]
}