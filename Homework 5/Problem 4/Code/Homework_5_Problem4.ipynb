{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7uRC0yQAUW9",
        "outputId": "cfe4ca4c-2077-4033-c246-4ba5eb7ce9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import math\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "########Reversed French → English dataset\n",
        "data = [\n",
        "    (\"I am cold\", \"J'ai froid\"),\n",
        "    (\"You are tired\", \"Tu es fatigué\"),\n",
        "    (\"He is hungry\", \"Il a faim\"),\n",
        "    (\"She is happy\", \"Elle est heureuse\"),\n",
        "    (\"We are friends\", \"Nous sommes amis\"),\n",
        "    (\"They are students\", \"Ils sont étudiants\"),\n",
        "    (\"The cat is sleeping\", \"Le chat dort\"),\n",
        "    (\"The sun is shining\", \"Le soleil brille\"),\n",
        "    (\"We love music\", \"Nous aimons la musique\"),\n",
        "    (\"She speaks French fluently\", \"Elle parle français couramment\"),\n",
        "    (\"He enjoys reading books\", \"Il aime lire des livres\"),\n",
        "    (\"They play soccer every weekend\", \"Ils jouent au football chaque week-end\"),\n",
        "    (\"The movie starts at 7 PM\", \"Le film commence à 19 heures\"),\n",
        "    (\"She wears a red dress\", \"Elle porte une robe rouge\"),\n",
        "    (\"We cook dinner together\", \"Nous cuisinons le dîner ensemble\"),\n",
        "    (\"He drives a blue car\", \"Il conduit une voiture bleue\"),\n",
        "    (\"They visit museums often\", \"Ils visitent souvent des musées\"),\n",
        "    (\"The restaurant serves delicious food\", \"Le restaurant sert une délicieuse cuisine\"),\n",
        "    (\"She studies mathematics at university\", \"Elle étudie les mathématiques à l'université\"),\n",
        "    (\"We watch movies on Fridays\", \"Nous regardons des films le vendredi\"),\n",
        "    (\"He listens to music while jogging\", \"Il écoute de la musique en faisant du jogging\"),\n",
        "    (\"They travel around the world\", \"Ils voyagent autour du monde\"),\n",
        "    (\"The book is on the table\", \"Le livre est sur la table\"),\n",
        "    (\"She dances gracefully\", \"Elle danse avec grâce\"),\n",
        "    (\"We celebrate birthdays with cake\", \"Nous célébrons les anniversaires avec un gâteau\"),\n",
        "    (\"He works hard every day\", \"Il travaille dur tous les jours\"),\n",
        "    (\"They speak different languages\", \"Ils parlent différentes langues\"),\n",
        "    (\"The flowers bloom in spring\", \"Les fleurs fleurissent au printemps\"),\n",
        "    (\"She writes poetry in her free time\", \"Elle écrit de la poésie pendant son temps libre\"),\n",
        "    (\"We learn something new every day\", \"Nous apprenons quelque chose de nouveau chaque jour\"),\n",
        "    (\"The dog barks loudly\", \"Le chien aboie bruyamment\"),\n",
        "    (\"He sings beautifully\", \"Il chante magnifiquement\"),\n",
        "    (\"They swim in the pool\", \"Ils nagent dans la piscine\"),\n",
        "    (\"The birds chirp in the morning\", \"Les oiseaux gazouillent le matin\"),\n",
        "    (\"She teaches English at school\", \"Elle enseigne l'anglais à l'école\"),\n",
        "    (\"We eat breakfast together\", \"Nous prenons le petit déjeuner ensemble\"),\n",
        "    (\"He paints landscapes\", \"Il peint des paysages\"),\n",
        "    (\"They laugh at the joke\", \"Ils rient de la blague\"),\n",
        "    (\"The clock ticks loudly\", \"L'horloge tic-tac bruyamment\"),\n",
        "    (\"She runs in the park\", \"Elle court dans le parc\"),\n",
        "    (\"We travel by train\", \"Nous voyageons en train\"),\n",
        "    (\"He writes a letter\", \"Il écrit une lettre\"),\n",
        "    (\"They read books at the library\", \"Ils lisent des livres à la bibliothèque\"),\n",
        "    (\"The baby cries\", \"Le bébé pleure\"),\n",
        "    (\"She studies hard for exams\", \"Elle étudie dur pour les examens\"),\n",
        "    (\"We plant flowers in the garden\", \"Nous plantons des fleurs dans le jardin\"),\n",
        "    (\"He fixes the car\", \"Il répare la voiture\"),\n",
        "    (\"They drink coffee in the morning\", \"Ils boivent du café le matin\"),\n",
        "    (\"The sun sets in the evening\", \"Le soleil se couche le soir\"),\n",
        "    (\"She dances at the party\", \"Elle danse à la fête\"),\n",
        "    (\"We play music at the concert\", \"Nous jouons de la musique au concert\"),\n",
        "    (\"He cooks dinner for his family\", \"Il cuisine le dîner pour sa famille\"),\n",
        "    (\"They study French grammar\", \"Ils étudient la grammaire française\"),\n",
        "    (\"The rain falls gently\", \"La pluie tombe doucement\"),\n",
        "    (\"She sings a song\", \"Elle chante une chanson\"),\n",
        "    (\"We watch a movie together\", \"Nous regardons un film ensemble\"),\n",
        "    (\"He sleeps deeply\", \"Il dort profondément\"),\n",
        "    (\"They travel to Paris\", \"Ils voyagent à Paris\"),\n",
        "    (\"The children play in the park\", \"Les enfants jouent dans le parc\"),\n",
        "    (\"She walks along the beach\", \"Elle se promène le long de la plage\"),\n",
        "    (\"We talk on the phone\", \"Nous parlons au téléphone\"),\n",
        "    (\"He waits for the bus\", \"Il attend le bus\"),\n",
        "    (\"They visit the Eiffel Tower\", \"Ils visitent la tour Eiffel\"),\n",
        "    (\"The stars twinkle at night\", \"Les étoiles scintillent la nuit\"),\n",
        "    (\"She dreams of flying\", \"Elle rêve de voler\"),\n",
        "    (\"We work in the office\", \"Nous travaillons au bureau\"),\n",
        "    (\"He studies history\", \"Il étudie l'histoire\"),\n",
        "    (\"They listen to the radio\", \"Ils écoutent la radio\"),\n",
        "    (\"The wind blows gently\", \"Le vent souffle doucement\"),\n",
        "    (\"She swims in the ocean\", \"Elle nage dans l'océan\"),\n",
        "    (\"We dance at the wedding\", \"Nous dansons au mariage\"),\n",
        "    (\"He climbs the mountain\", \"Il gravit la montagne\"),\n",
        "    (\"They hike in the forest\", \"Ils font de la randonnée dans la forêt\"),\n",
        "    (\"The cat meows loudly\", \"Le chat miaule bruyamment\"),\n",
        "    (\"She paints a picture\", \"Elle peint un tableau\"),\n",
        "    (\"We build a sandcastle\", \"Nous construisons un château de sable\"),\n",
        "    (\"He sings in the choir\", \"Il chante dans le chœur\"),\n",
        "    (\"They ride bicycles\", \"Ils font du vélo\"),\n",
        "    (\"The coffee is hot\", \"Le café est chaud\"),\n",
        "    (\"She wears glasses\", \"Elle porte des lunettes\"),\n",
        "    (\"We visit our grandparents\", \"Nous rendons visite à nos grands-parents\"),\n",
        "    (\"He plays the guitar\", \"Il joue de la guitare\"),\n",
        "    (\"They go shopping\", \"Ils font du shopping\"),\n",
        "    (\"The teacher explains the lesson\", \"Le professeur explique la leçon\"),\n",
        "    (\"She takes the train to work\", \"Elle prend le train pour aller au travail\"),\n",
        "    (\"We bake cookies\", \"Nous faisons des biscuits\"),\n",
        "    (\"He washes his hands\", \"Il se lave les mains\"),\n",
        "    (\"They enjoy the sunset\", \"Ils apprécient le coucher du soleil\"),\n",
        "    (\"The river flows calmly\", \"La rivière coule calmement\"),\n",
        "    (\"She feeds the cat\", \"Elle nourrit le chat\"),\n",
        "    (\"We visit the museum\", \"Nous visitons le musée\"),\n",
        "    (\"He fixes his bicycle\", \"Il répare son vélo\"),\n",
        "    (\"They paint the walls\", \"Ils peignent les murs\"),\n",
        "    (\"The baby sleeps peacefully\", \"Le bébé dort paisiblement\"),\n",
        "    (\"She ties her shoelaces\", \"Elle attache ses lacets\"),\n",
        "    (\"We climb the stairs\", \"Nous montons les escaliers\"),\n",
        "    (\"He shaves in the morning\", \"Il se rase le matin\"),\n",
        "    (\"They set the table\", \"Ils mettent la table\"),\n",
        "    (\"The airplane takes off\", \"L'avion décolle\"),\n",
        "    (\"She waters the plants\", \"Elle arrose les plantes\"),\n",
        "    (\"We practice yoga\", \"Nous pratiquons le yoga\"),\n",
        "    (\"He turns off the light\", \"Il éteint la lumière\"),\n",
        "    (\"They play video games\", \"Ils jouent aux jeux vidéo\"),\n",
        "    (\"The soup smells delicious\", \"La soupe sent délicieusement bon\"),\n",
        "    (\"She locks the door\", \"Elle ferme la porte à clé\"),\n",
        "    (\"We enjoy a picnic\", \"Nous profitons d'un pique-nique\"),\n",
        "    (\"He checks his email\", \"Il vérifie ses emails\"),\n",
        "    (\"They go to the gym\", \"Ils vont à la salle de sport\"),\n",
        "    (\"The moon shines brightly\", \"La lune brille intensément\"),\n",
        "    (\"She catches the bus\", \"Elle attrape le bus\"),\n",
        "    (\"We greet our neighbors\", \"Nous saluons nos voisins\"),\n",
        "    (\"He combs his hair\", \"Il se peigne les cheveux\"),\n",
        "    (\"They wave goodbye\", \"Ils font un signe d'adieu\")\n",
        "]\n",
        "\n",
        "#Tokenizer & Vocab\n",
        "def tokenize(text):\n",
        "    return text.lower().strip().split()\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, texts, specials=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]):\n",
        "        tokens = [tok for sent in texts for tok in tokenize(sent)]\n",
        "        self.itos = specials + sorted(set(tokens))\n",
        "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "        self.pad = self.stoi[\"<pad>\"]\n",
        "        self.bos = self.stoi[\"<bos>\"]\n",
        "        self.eos = self.stoi[\"<eos>\"]\n",
        "        self.unk = self.stoi[\"<unk>\"]\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.bos] + [self.stoi.get(tok, self.unk) for tok in tokenize(text)] + [self.eos]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return \" \".join([self.itos[i] for i in ids if i not in [self.bos, self.eos, self.pad]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "fr_vocab = Vocab([fr for fr, en in data])  # source\n",
        "en_vocab = Vocab([en for fr, en in data])  # target\n",
        "\n",
        "#Dataset & Collate\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fr, en = self.pairs[idx]\n",
        "        return torch.tensor(fr_vocab.encode(fr)), torch.tensor(en_vocab.encode(en))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src, tgt = zip(*batch)\n",
        "    src = pad_sequence(src, batch_first=True, padding_value=fr_vocab.pad)\n",
        "    tgt = pad_sequence(tgt, batch_first=True, padding_value=en_vocab.pad)\n",
        "    return src.to(device), tgt.to(device)\n",
        "\n",
        "dataset = TranslationDataset(data)\n",
        "train_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size, max_len=5000):\n",
        "        super().__init__()\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, emb_size, 2) * (-math.log(10000.0) / emb_size))\n",
        "        pe = torch.zeros(max_len, emb_size)\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.pe = pe.unsqueeze(0).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "cdVLvXILCjN-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######Transformer Encoder-Decoder for FR → EN\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, fr_vocab_size, en_vocab_size, emb_size=128, num_heads=2, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.src_emb = nn.Embedding(fr_vocab_size, emb_size)\n",
        "        self.tgt_emb = nn.Embedding(en_vocab_size, emb_size)\n",
        "        self.pos_enc = PositionalEncoding(emb_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=512,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(emb_size, en_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.transformer.generate_square_subsequent_mask(src.size(1)).to(device)\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
        "        src = self.pos_enc(self.src_emb(src))\n",
        "        tgt = self.pos_enc(self.tgt_emb(tgt))\n",
        "        out = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "y7ehH3Z3CtMF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####Training Function\n",
        "\n",
        "def train_seq2seq(model, loader, epochs=5):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=en_vocab.pad)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for src, tgt in loader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_expected = tgt[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_expected = tgt_expected.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_expected)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(1) == tgt_expected).sum().item()\n",
        "            total += tgt_expected.ne(en_vocab.pad).sum().item()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f} | Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "pabpXU7RC0GZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Translate French → English\n",
        "\n",
        "def translate(model, sentence, max_len=20):\n",
        "    model.eval()\n",
        "    src = torch.tensor(fr_vocab.encode(sentence)).unsqueeze(0).to(device)\n",
        "    tgt = torch.tensor([[en_vocab.bos]]).to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out = model(src, tgt)\n",
        "            next_token = out[:, -1, :].argmax(dim=1, keepdim=True)\n",
        "            tgt = torch.cat([tgt, next_token], dim=1)\n",
        "            if next_token.item() == en_vocab.eos:\n",
        "                break\n",
        "\n",
        "    return en_vocab.decode(tgt[0].tolist())\n"
      ],
      "metadata": {
        "id": "2FA8A7UtC5E9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerModel(len(fr_vocab), len(en_vocab), emb_size=128, num_heads=2, num_layers=2)\n",
        "train_seq2seq(model, train_loader, epochs=10)\n",
        "\n",
        "print(\"\\nSample Translations (FR → EN):\")\n",
        "print(\"FR: Elle est heureuse\\nEN:\", translate(model, \"Elle est heureuse\"))\n",
        "print(\"FR: Nous sommes amis\\nEN:\", translate(model, \"Nous sommes amis\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc4AQnr2C9Dd",
        "outputId": "0c57f500-4475-443e-88df-6876057a0b66"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 273.9030 | Accuracy: 0.1889\n",
            "Epoch 2 | Loss: 240.2113 | Accuracy: 0.2504\n",
            "Epoch 3 | Loss: 208.8282 | Accuracy: 0.3379\n",
            "Epoch 4 | Loss: 182.4458 | Accuracy: 0.3656\n",
            "Epoch 5 | Loss: 162.0054 | Accuracy: 0.3932\n",
            "Epoch 6 | Loss: 138.7531 | Accuracy: 0.4516\n",
            "Epoch 7 | Loss: 117.4223 | Accuracy: 0.5084\n",
            "Epoch 8 | Loss: 101.2934 | Accuracy: 0.6006\n",
            "Epoch 9 | Loss: 89.3477 | Accuracy: 0.6359\n",
            "Epoch 10 | Loss: 78.4340 | Accuracy: 0.6605\n",
            "\n",
            "Sample Translations (FR → EN):\n",
            "FR: Elle est heureuse\n",
            "EN: la lune brille intensément\n",
            "FR: Nous sommes amis\n",
            "EN: la lune brille intensément\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########Train and Evaluate 8 Transformer Configs#####"
      ],
      "metadata": {
        "id": "R6_DbfO_Dklq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_transformer_fr_to_en_configs():\n",
        "    results = []\n",
        "    configs = [(l, h) for l in [1, 2, 4] for h in [2, 4]]\n",
        "\n",
        "    for num_layers, num_heads in configs:\n",
        "        print(f\"\\n===== Layers: {num_layers}, Heads: {num_heads} =====\")\n",
        "        model = TransformerModel(\n",
        "            fr_vocab_size=len(fr_vocab),\n",
        "            en_vocab_size=len(en_vocab),\n",
        "            emb_size=128,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        start = time.time()\n",
        "        train_seq2seq(model, train_loader, epochs=5)\n",
        "        duration = time.time() - start\n",
        "\n",
        "        # Qualitative result\n",
        "        translation = translate(model, \"Nous sommes amis\")\n",
        "\n",
        "        results.append({\n",
        "            \"layers\": num_layers,\n",
        "            \"heads\": num_heads,\n",
        "            \"params\": sum(p.numel() for p in model.parameters()),\n",
        "            \"time\": round(duration, 2),\n",
        "            \"sample\": translation\n",
        "        })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "bvDoZMMmDpR2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_fr_en_results = run_transformer_fr_to_en_configs()\n",
        "\n",
        "print(\"\\n=== Summary of FR → EN Transformer Configs ===\")\n",
        "for res in transformer_fr_en_results:\n",
        "    print(f\"Layers: {res['layers']} | Heads: {res['heads']} | Params: {res['params']:,} | Time: {res['time']}s\")\n",
        "    print(\"Translation (FR → EN):\", res['sample'])\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKoNhOfPDsy5",
        "outputId": "0fdfc782-c75b-4a90-c4f1-f9b96bb13f2a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Layers: 1, Heads: 2 =====\n",
            "Epoch 1 | Loss: 275.7745 | Accuracy: 0.1935\n",
            "Epoch 2 | Loss: 214.3554 | Accuracy: 0.3349\n",
            "Epoch 3 | Loss: 175.5939 | Accuracy: 0.3917\n",
            "Epoch 4 | Loss: 142.8398 | Accuracy: 0.4485\n",
            "Epoch 5 | Loss: 118.2405 | Accuracy: 0.5499\n",
            "\n",
            "===== Layers: 1, Heads: 4 =====\n",
            "Epoch 1 | Loss: 272.9899 | Accuracy: 0.1905\n",
            "Epoch 2 | Loss: 219.8911 | Accuracy: 0.2796\n",
            "Epoch 3 | Loss: 177.8433 | Accuracy: 0.3794\n",
            "Epoch 4 | Loss: 141.0090 | Accuracy: 0.4747\n",
            "Epoch 5 | Loss: 113.8762 | Accuracy: 0.5607\n",
            "\n",
            "===== Layers: 2, Heads: 2 =====\n",
            "Epoch 1 | Loss: 278.2978 | Accuracy: 0.1935\n",
            "Epoch 2 | Loss: 239.2145 | Accuracy: 0.2780\n",
            "Epoch 3 | Loss: 212.1012 | Accuracy: 0.3456\n",
            "Epoch 4 | Loss: 192.1183 | Accuracy: 0.3518\n",
            "Epoch 5 | Loss: 169.7797 | Accuracy: 0.3687\n",
            "\n",
            "===== Layers: 2, Heads: 4 =====\n",
            "Epoch 1 | Loss: 278.1284 | Accuracy: 0.1659\n",
            "Epoch 2 | Loss: 244.5136 | Accuracy: 0.2227\n",
            "Epoch 3 | Loss: 214.6913 | Accuracy: 0.3103\n",
            "Epoch 4 | Loss: 187.5665 | Accuracy: 0.3794\n",
            "Epoch 5 | Loss: 161.4435 | Accuracy: 0.3994\n",
            "\n",
            "===== Layers: 4, Heads: 2 =====\n",
            "Epoch 1 | Loss: 285.6269 | Accuracy: 0.1690\n",
            "Epoch 2 | Loss: 269.6719 | Accuracy: 0.1736\n",
            "Epoch 3 | Loss: 258.0960 | Accuracy: 0.1920\n",
            "Epoch 4 | Loss: 251.7839 | Accuracy: 0.2151\n",
            "Epoch 5 | Loss: 263.3688 | Accuracy: 0.1767\n",
            "\n",
            "===== Layers: 4, Heads: 4 =====\n",
            "Epoch 1 | Loss: 283.7683 | Accuracy: 0.1674\n",
            "Epoch 2 | Loss: 270.4275 | Accuracy: 0.1736\n",
            "Epoch 3 | Loss: 266.0503 | Accuracy: 0.1736\n",
            "Epoch 4 | Loss: 262.8269 | Accuracy: 0.1751\n",
            "Epoch 5 | Loss: 252.4207 | Accuracy: 0.2012\n",
            "\n",
            "=== Summary of FR → EN Transformer Configs ===\n",
            "Layers: 1 | Heads: 2 | Params: 566,035 | Time: 2.48s\n",
            "Translation (FR → EN): il se peigne\n",
            "------------------------------------------------------------\n",
            "Layers: 1 | Heads: 4 | Params: 566,035 | Time: 2.46s\n",
            "Translation (FR → EN): elle étudie les mathématiques à clé\n",
            "------------------------------------------------------------\n",
            "Layers: 2 | Heads: 2 | Params: 1,028,883 | Time: 4.19s\n",
            "Translation (FR → EN): ils font du café\n",
            "------------------------------------------------------------\n",
            "Layers: 2 | Heads: 4 | Params: 1,028,883 | Time: 4.06s\n",
            "Translation (FR → EN): elle porte le chat\n",
            "------------------------------------------------------------\n",
            "Layers: 4 | Heads: 2 | Params: 1,954,579 | Time: 7.46s\n",
            "Translation (FR → EN): \n",
            "------------------------------------------------------------\n",
            "Layers: 4 | Heads: 4 | Params: 1,954,579 | Time: 7.4s\n",
            "Translation (FR → EN): nous danse\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######RNN Encoder-Decoder for FR → EN\n",
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_size, emb_size)\n",
        "        self.rnn = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        outputs, hidden = self.rnn(emb)\n",
        "        return outputs, hidden\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, output_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_size, emb_size)\n",
        "        self.rnn = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        emb = self.embedding(x)\n",
        "        out, hidden = self.rnn(emb, hidden)\n",
        "        return self.fc(out), hidden\n",
        "\n",
        "class Seq2SeqRNN(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        _, hidden = self.encoder(src)\n",
        "        output, _ = self.decoder(tgt, hidden)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "iJ-PWdZbEAKX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Train Function for RNN\n",
        "\n",
        "def train_rnn(model, loader, epochs=5):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=en_vocab.pad)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for src, tgt in loader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_expected = tgt[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_expected = tgt_expected.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_expected)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(1) == tgt_expected).sum().item()\n",
        "            total += tgt_expected.ne(en_vocab.pad).sum().item()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f} | Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "GC_AY6FgEIWR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Translate FR → EN (RNN no attention)\n",
        "\n",
        "def translate_rnn(model, sentence, max_len=20):\n",
        "    model.eval()\n",
        "    src = torch.tensor(fr_vocab.encode(sentence)).unsqueeze(0).to(device)\n",
        "    tgt = torch.tensor([[en_vocab.bos]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, hidden = model.encoder(src)\n",
        "        for _ in range(max_len):\n",
        "            out, hidden = model.decoder(tgt[:, -1:], hidden)\n",
        "            next_token = out[:, -1, :].argmax(1, keepdim=True)\n",
        "            tgt = torch.cat([tgt, next_token], dim=1)\n",
        "            if next_token.item() == en_vocab.eos:\n",
        "                break\n",
        "\n",
        "    return en_vocab.decode(tgt[0].tolist())\n"
      ],
      "metadata": {
        "id": "XJ1uM_1sEOMs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Run RNN Model (FR → EN)\n",
        "\n",
        "emb_size = 128\n",
        "hidden_size = 256\n",
        "\n",
        "rnn_enc = RNNEncoder(len(fr_vocab), emb_size, hidden_size)\n",
        "rnn_dec = RNNDecoder(len(en_vocab), emb_size, hidden_size)\n",
        "rnn_model = Seq2SeqRNN(rnn_enc, rnn_dec)\n",
        "\n",
        "train_rnn(rnn_model, train_loader, epochs=10)\n",
        "\n",
        "print(\"\\nSample RNN Translations (FR → EN):\")\n",
        "print(\"FR: Elle est heureuse\\nEN:\", translate_rnn(rnn_model, \"Elle est heureuse\"))\n",
        "print(\"FR: Nous sommes amis\\nEN:\", translate_rnn(rnn_model, \"Nous sommes amis\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKke6vUMER0F",
        "outputId": "e306c72e-0d8e-42f3-d744-b5b3bf8618de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 274.0249 | Accuracy: 0.1751\n",
            "Epoch 2 | Loss: 208.5327 | Accuracy: 0.2934\n",
            "Epoch 3 | Loss: 161.9120 | Accuracy: 0.3902\n",
            "Epoch 4 | Loss: 124.8780 | Accuracy: 0.4639\n",
            "Epoch 5 | Loss: 96.1698 | Accuracy: 0.5853\n",
            "Epoch 6 | Loss: 71.4778 | Accuracy: 0.7051\n",
            "Epoch 7 | Loss: 52.0005 | Accuracy: 0.7757\n",
            "Epoch 8 | Loss: 38.3043 | Accuracy: 0.8433\n",
            "Epoch 9 | Loss: 28.5458 | Accuracy: 0.9078\n",
            "Epoch 10 | Loss: 20.9452 | Accuracy: 0.9539\n",
            "\n",
            "Sample RNN Translations (FR → EN):\n",
            "FR: Elle est heureuse\n",
            "EN: elle attache ses lacets\n",
            "FR: Nous sommes amis\n",
            "EN: elle attache ses lacets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Attention Module\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch, 1, hidden], encoder_outputs: [batch, seq_len, hidden]\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "\n",
        "        hidden = hidden.repeat(1, seq_len, 1)  # [batch, seq_len, hidden]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        energy = energy.transpose(1, 2)  # [batch, hidden, seq_len]\n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)  # [batch, 1, hidden]\n",
        "        attn_weights = torch.bmm(v, energy).squeeze(1)  # [batch, seq_len]\n",
        "        return torch.softmax(attn_weights, dim=1)  # [batch, seq_len]\n"
      ],
      "metadata": {
        "id": "ZsEnq2loElhd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Decoder with Attention\n",
        "\n",
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, output_size, emb_size, hidden_size, attention):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_size, emb_size)\n",
        "        self.rnn = nn.GRU(hidden_size + emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "        self.attention = attention\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(x)  # [batch, 1, emb]\n",
        "        attn_weights = self.attention(hidden.permute(1, 0, 2), encoder_outputs)  # [batch, seq_len]\n",
        "        attn_weights = attn_weights.unsqueeze(1)  # [batch, 1, seq_len]\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)  # [batch, 1, hidden]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)  # [batch, 1, emb + hidden]\n",
        "        output, hidden = self.rnn(rnn_input, hidden)  # [batch, 1, hidden]\n",
        "        output = self.fc(torch.cat((output, context), dim=2))  # [batch, 1, vocab]\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "4T69bCbMEpLl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Seq2Seq Model with Attention\n",
        "\n",
        "class Seq2SeqAttn(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(tgt.size(1)):\n",
        "            out, hidden = self.decoder(tgt[:, t].unsqueeze(1), hidden, encoder_outputs)\n",
        "            outputs.append(out)\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "0IQjFNbNEtAU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Training Function\n",
        "\n",
        "def train_rnn_attention(model, loader, epochs=5):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=en_vocab.pad)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for src, tgt in loader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_expected = tgt[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_expected = tgt_expected.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_expected)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(1) == tgt_expected).sum().item()\n",
        "            total += tgt_expected.ne(en_vocab.pad).sum().item()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f} | Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "N7EXILNvEw2t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Translate with RNN + Attention\n",
        "\n",
        "def translate_rnn_attention(model, sentence, max_len=20):\n",
        "    model.eval()\n",
        "    src = torch.tensor(fr_vocab.encode(sentence)).unsqueeze(0).to(device)\n",
        "    tgt = torch.tensor([[en_vocab.bos]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src)\n",
        "        for _ in range(max_len):\n",
        "            out, hidden = model.decoder(tgt[:, -1:], hidden, encoder_outputs)\n",
        "            next_token = out[:, -1, :].argmax(1, keepdim=True)\n",
        "            tgt = torch.cat([tgt, next_token], dim=1)\n",
        "            if next_token.item() == en_vocab.eos:\n",
        "                break\n",
        "\n",
        "    return en_vocab.decode(tgt[0].tolist())\n"
      ],
      "metadata": {
        "id": "3VSl7wRpEzIN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Run Model (FR → EN)\n",
        "\n",
        "emb_size = 128\n",
        "hidden_size = 256\n",
        "\n",
        "attn = Attention(hidden_size)\n",
        "rnn_enc_attn = RNNEncoder(len(fr_vocab), emb_size, hidden_size)\n",
        "rnn_dec_attn = AttnDecoder(len(en_vocab), emb_size, hidden_size, attn)\n",
        "rnn_attn_model = Seq2SeqAttn(rnn_enc_attn, rnn_dec_attn)\n",
        "\n",
        "train_rnn_attention(rnn_attn_model, train_loader, epochs=10)\n",
        "\n",
        "print(\"\\nSample RNN + Attention Translations (FR → EN):\")\n",
        "print(\"FR: Elle est heureuse\\nEN:\", translate_rnn_attention(rnn_attn_model, \"Elle est heureuse\"))\n",
        "print(\"FR: Nous sommes amis\\nEN:\", translate_rnn_attention(rnn_attn_model, \"Nous sommes amis\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TG-h_j1E359",
        "outputId": "62b2e447-2963-4aae-a041-9f9d4f0aad46"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 269.2495 | Accuracy: 0.2120\n",
            "Epoch 2 | Loss: 210.8001 | Accuracy: 0.3026\n",
            "Epoch 3 | Loss: 170.0229 | Accuracy: 0.3533\n",
            "Epoch 4 | Loss: 132.1062 | Accuracy: 0.4393\n",
            "Epoch 5 | Loss: 102.8654 | Accuracy: 0.5376\n",
            "Epoch 6 | Loss: 79.1983 | Accuracy: 0.6436\n",
            "Epoch 7 | Loss: 59.2340 | Accuracy: 0.7373\n",
            "Epoch 8 | Loss: 45.8380 | Accuracy: 0.7896\n",
            "Epoch 9 | Loss: 35.7974 | Accuracy: 0.8310\n",
            "Epoch 10 | Loss: 29.4840 | Accuracy: 0.8664\n",
            "\n",
            "Sample RNN + Attention Translations (FR → EN):\n",
            "FR: Elle est heureuse\n",
            "EN: il se peigne les cheveux\n",
            "FR: Nous sommes amis\n",
            "EN: il se peigne les cheveux\n"
          ]
        }
      ]
    }
  ]
}